{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "CEiffH-_LcEZ",
        "outputId": "839a1f0d-a8af-4932-fd65-a6601f037a56"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import string\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler # Import TensorDataset\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "import emoji\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "class Preprocess:\n",
        "    def remove_non_letters(self, text):\n",
        "        return re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    def remove_stop_words(self, text):\n",
        "        stop_words = set(stopwords.words('english')) - {\"never\", \"not\", \"nor\"}\n",
        "        tokens = word_tokenize(text)\n",
        "        filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
        "        return ' '.join(filtered_words)\n",
        "\n",
        "    def normalize_words(self, text):\n",
        "        return text.lower()\n",
        "\n",
        "    def remove_short_words(self, text, min_length=2):\n",
        "        return ' '.join([word for word in text.split() if len(word) >= min_length])\n",
        "\n",
        "    def remove_long_words(self, text, max_length=15):\n",
        "        return ' '.join([word for word in text.split() if len(word) <= max_length])\n",
        "\n",
        "    def get_wordnet_pos(self, tag):\n",
        "        if tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return wordnet.NOUN\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text) \n",
        "        text = re.sub(r'<.*?>', '', text)  \n",
        "        text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  \n",
        "        return text\n",
        "\n",
        "    def remove_twitter_handles(self, text):\n",
        "        return re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    def expand_contractions(self, text):\n",
        "        contractions_dict = {\n",
        "            \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"aren't\": \"are not\",\n",
        "            \"can't\": \"cannot\", \"couldn't\": \"could not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
        "            \"haven't\": \"have not\", \"he's\": \"he is\", \"I'm\": \"I am\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
        "            \"mightn't\": \"might not\", \"mustn't\": \"must not\", \"shan't\": \"shall not\", \"she's\": \"she is\",\n",
        "            \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\", \"they're\": \"they are\",\n",
        "            \"we're\": \"we are\", \"weren't\": \"were not\", \"who's\": \"who is\", \"won't\": \"will not\",\n",
        "            \"wouldn't\": \"would not\", \"you're\": \"you are\", \"you've\": \"you have\", \"iam\": \"i am\"\n",
        "        }\n",
        "        contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
        "                                          flags=re.IGNORECASE|re.DOTALL)\n",
        "\n",
        "        def replace(match):\n",
        "            return contractions_dict[match.group(0).lower()]\n",
        "\n",
        "        return contractions_pattern.sub(replace, text)\n",
        "\n",
        "    def remove_numbers(self, text):\n",
        "        return re.sub(r'\\d+', '', text)\n",
        "\n",
        "    def remove_emoticons(self, text):\n",
        "        return emoji.replace_emoji(text, replace='')\n",
        "\n",
        "    def expand_abbreviations(self, text):\n",
        "        abbreviations_dict = {\n",
        "            \"btw\": \"by the way\", \"lol\": \"laughing out loud\", \"idk\": \"I don't know\", \"omg\": \"oh my god\",\n",
        "            \"brb\": \"be right back\", \"imo\": \"in my opinion\", \"smh\": \"shaking my head\", \"tbh\": \"to be honest\"\n",
        "        }\n",
        "        abbreviations_pattern = re.compile('({})'.format('|'.join(abbreviations_dict.keys())), flags=re.IGNORECASE)\n",
        "\n",
        "        def replace_abbreviation(match):\n",
        "            return abbreviations_dict[match.group(0).lower()]\n",
        "\n",
        "        return abbreviations_pattern.sub(replace_abbreviation, text)\n",
        "\n",
        "    def remove_extra_spaces(self, text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_csv('train.csv')  \n",
        "\n",
        "\n",
        "PP = Preprocess()\n",
        "\n",
        "data['cleaned_text'] = data['text'].apply(PP.remove_non_letters)\n",
        "data['cleaned_text'] = data['cleaned_text'].apply(PP.remove_stop_words)\n",
        "data['cleaned_text'] = data['cleaned_text'].apply(PP.normalize_words)\n",
        "data['cleaned_text'] = data['cleaned_text'].apply(PP.remove_short_words)\n",
        "data['cleaned_text'] = data['cleaned_text'].apply(lambda x: PP.remove_long_words(x, 15))\n",
        "data['cleaned_text'] = data['cleaned_text'].apply(PP.remove_twitter_handles)\n",
        "data['cleaned_text'] = data['cleaned_text'].apply(PP.clean_text)\n",
        "data['cleaned_text'] = data['cleaned_text'].apply(PP.expand_contractions)\n",
        "data['cleaned_text'] = data['cleaned_text'].apply(PP.remove_numbers)\n",
        "data['cleaned_text'] = data['cleaned_text'].apply(PP.remove_stop_words)\n",
        "data['cleaned_text'] = data['cleaned_text'].apply(PP.remove_short_words)\n",
        "data['cleaned_text'] = data['cleaned_text'].apply(PP.remove_emoticons) \n",
        "data['cleaned_text'] = data['cleaned_text'].apply(PP.expand_abbreviations) \n",
        "data['cleaned_text'] = data['cleaned_text'].apply(PP.remove_extra_spaces)\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "MAX_LEN = 128  \n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in data['cleaned_text']:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text,  \n",
        "        add_special_tokens=True,  \n",
        "        max_length=MAX_LEN, \n",
        "        pad_to_max_length=True, \n",
        "        return_attention_mask=True,  \n",
        "        return_tensors='pt',  \n",
        "    )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(data['target'].values)\n",
        "\n",
        "\n",
        "K = 5\n",
        "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "for train_idx, test_idx in skf.split(input_ids, labels):\n",
        "    train_inputs, val_inputs = input_ids[train_idx], input_ids[test_idx]\n",
        "    train_masks, val_masks = attention_masks[train_idx], attention_masks[test_idx]\n",
        "    train_labels, val_labels = labels[train_idx], labels[test_idx]\n",
        "\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=16)\n",
        "\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        num_labels=2,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "    )\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "    epochs = 3\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train() \n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "    predictions = np.concatenate(predictions)\n",
        "    true_labels = np.concatenate(true_labels)\n",
        "\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    print(f'Bert Fold Validation Accuracy: {accuracy:.4f}')\n",
        "\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, predictions))\n",
        "\n",
        "\n",
        "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjj5NKFrHLcD"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "for batch in val_dataloader:\n",
        "    b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "    true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "predictions = np.concatenate(predictions)\n",
        "true_labels = np.concatenate(true_labels)\n",
        "\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n",
        "\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, predictions))\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIxjWunWlxSp"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "from transformers import DebertaTokenizer, DebertaForSequenceClassification\n",
        "\n",
        "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
        "model = DebertaForSequenceClassification.from_pretrained(\n",
        "    \"microsoft/deberta-base\",\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "\n",
        "MAX_LEN = 128\n",
        "input_ids, attention_masks = [], []\n",
        "\n",
        "for text in data['cleaned_text']:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_LEN,\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(data['target'].values)\n",
        "\n",
        "\n",
        "K = 5\n",
        "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "for train_idx, test_idx in skf.split(input_ids, labels):\n",
        "    train_inputs, val_inputs = input_ids[train_idx], input_ids[test_idx]\n",
        "    train_masks, val_masks = attention_masks[train_idx], attention_masks[test_idx]\n",
        "    train_labels, val_labels = labels[train_idx], labels[test_idx]\n",
        "\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=16)\n",
        "\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "    epochs = 3\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    # GPU or CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #memory management\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_dataloader:\n",
        "            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "            model.zero_grad()\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_dataloader)}\")\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "    predictions = np.concatenate(predictions)\n",
        "    true_labels = np.concatenate(true_labels)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    print(f\"DeBert Fold Validation Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfUJQPbJwI-0"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "\n",
        "fold = 0 \n",
        "\n",
        "for batch in val_dataloader:\n",
        "    b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "    logits = outputs.logits\n",
        "    predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "    true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "predictions = np.concatenate(predictions)\n",
        "true_labels = np.concatenate(true_labels)\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "\n",
        "print(f\"DeBERT Fold {fold+1} Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "print(f\"Classification Report for Fold {fold+1}:\\n\")\n",
        "report = classification_report(true_labels, predictions, target_names=['Class 0', 'Class 1'])\n",
        "print(report)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "print(f\"Confusion Matrix for Fold {fold+1}:\\n\", cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nLUvE9iwJuT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "data = pd.read_csv('train.csv')  \n",
        "data['cleaned_text'] = data['text']  \n",
        "\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2,  \n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False,\n",
        ")\n",
        "\n",
        "\n",
        "MAX_LEN = 128  \n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in data['cleaned_text']:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text,  \n",
        "        add_special_tokens=True,  \n",
        "        max_length=MAX_LEN,  \n",
        "        pad_to_max_length=True,  \n",
        "        return_attention_mask=True,  \n",
        "        return_tensors='pt',  \n",
        "    )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(data['target'].values)\n",
        "\n",
        "\n",
        "K = 5\n",
        "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "for train_idx, test_idx in skf.split(input_ids, labels):\n",
        "    train_inputs, val_inputs = input_ids[train_idx], input_ids[test_idx]\n",
        "    train_masks, val_masks = attention_masks[train_idx], attention_masks[test_idx]\n",
        "    train_labels, val_labels = labels[train_idx], labels[test_idx]\n",
        "\n",
        "\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=16)\n",
        "\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "    epochs = 3\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        train_predictions, train_true_labels = [], []\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "            train_predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            train_true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "\n",
        "        train_predictions = np.concatenate(train_predictions)\n",
        "        train_true_labels = np.concatenate(train_true_labels)\n",
        "        train_accuracy = accuracy_score(train_true_labels, train_predictions)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_predictions, val_true_labels = [], []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        val_predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        val_true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "    val_predictions = np.concatenate(val_predictions)\n",
        "    val_true_labels = np.concatenate(val_true_labels)\n",
        "\n",
        "\n",
        "    val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
        "    print(f\"Distilbert Fold Validation Accuracy: {val_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah6kvdt9OqaU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import DebertaTokenizer, DebertaForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
        "\n",
        "model = DebertaForSequenceClassification.from_pretrained(\n",
        "    \"microsoft/deberta-base\",\n",
        "    num_labels=2,  \n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False,\n",
        ")\n",
        "\n",
        "\n",
        "MAX_LEN = 128  \n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in data['cleaned_text']:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text,  \n",
        "        add_special_tokens=True,  \n",
        "        max_length=MAX_LEN, \n",
        "        pad_to_max_length=True,  \n",
        "        return_attention_mask=True,  \n",
        "        return_tensors='pt',  \n",
        "    )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(data['target'].values)\n",
        "\n",
        "\n",
        "K = 5\n",
        "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "for train_idx, test_idx in skf.split(input_ids, labels):\n",
        "    train_inputs, val_inputs = input_ids[train_idx], input_ids[test_idx]\n",
        "    train_masks, val_masks = attention_masks[train_idx], attention_masks[test_idx]\n",
        "    train_labels, val_labels = labels[train_idx], labels[test_idx]\n",
        "\n",
        "\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=16)\n",
        "\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "    epochs = 3\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        train_predictions, train_true_labels = [], []\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "            train_predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            train_true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "   \n",
        "        train_predictions = np.concatenate(train_predictions)\n",
        "        train_true_labels = np.concatenate(train_true_labels)\n",
        "        train_accuracy = accuracy_score(train_true_labels, train_predictions)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_predictions, val_true_labels = [], []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        val_predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        val_true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "    val_predictions = np.concatenate(val_predictions)\n",
        "    val_true_labels = np.concatenate(val_true_labels)\n",
        "\n",
        "\n",
        "    val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
        "    print(f\"Deberta Fold Validation Accuracy: {val_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Xj8I8Gq7dIQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "model.eval()\n",
        "val_predictions, val_true_labels = [], []\n",
        "\n",
        "for batch in val_dataloader:\n",
        "    b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    val_predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "    val_true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "val_predictions = np.concatenate(val_predictions)\n",
        "val_true_labels = np.concatenate(val_true_labels)\n",
        "\n",
        "\n",
        "val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
        "print(f\"Deberta Fold Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "report = classification_report(val_true_labels, val_predictions, target_names=['Class 0', 'Class 1'])\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "\n",
        "conf_matrix = confusion_matrix(val_true_labels, val_predictions)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hMc_n1SjL8b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2,  \n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False,\n",
        ")\n",
        "\n",
        "\n",
        "MAX_LEN = 128  \n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in data['cleaned_text']:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text, \n",
        "        add_special_tokens=True,  \n",
        "        max_length=MAX_LEN, \n",
        "        pad_to_max_length=True,  \n",
        "        return_attention_mask=True,  \n",
        "        return_tensors='pt', \n",
        "    )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(data['target'].values)\n",
        "\n",
        "\n",
        "K = 5\n",
        "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "for train_idx, test_idx in skf.split(input_ids, labels):\n",
        "    train_inputs, val_inputs = input_ids[train_idx], input_ids[test_idx]\n",
        "    train_masks, val_masks = attention_masks[train_idx], attention_masks[test_idx]\n",
        "    train_labels, val_labels = labels[train_idx], labels[test_idx]\n",
        "\n",
        "\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=16)\n",
        "\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "    epochs = 3\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        train_predictions, train_true_labels = [], []\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "            train_predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            train_true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "\n",
        "        train_predictions = np.concatenate(train_predictions)\n",
        "        train_true_labels = np.concatenate(train_true_labels)\n",
        "        train_accuracy = accuracy_score(train_true_labels, train_predictions)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_predictions, val_true_labels = [], []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        val_predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        val_true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "    val_predictions = np.concatenate(val_predictions)\n",
        "    val_true_labels = np.concatenate(val_true_labels)\n",
        "\n",
        "\n",
        "    val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
        "    print(f\"Fold Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "for batch in val_dataloader:\n",
        "    b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    loss = outputs.loss\n",
        "    val_loss += loss.item()\n",
        "\n",
        "    predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "    true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "avg_val_loss = val_loss / len(val_dataloader)\n",
        "predictions = np.concatenate(predictions)\n",
        "true_labels = np.concatenate(true_labels)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"Fold Validation Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Validation Loss: {avg_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s53UqdX8O0v"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "for batch in val_dataloader:\n",
        "    b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    loss = outputs.loss\n",
        "    val_loss += loss.item()\n",
        "\n",
        "    predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "    true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "avg_val_loss = val_loss / len(val_dataloader)\n",
        "predictions = np.concatenate(predictions)\n",
        "true_labels = np.concatenate(true_labels)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"Fold Validation Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, predictions, target_names=['Class 0', 'Class 1']))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(true_labels, predictions))\n",
        "\n",
        "\n",
        "train_val_diff = abs(train_accuracy - accuracy)\n",
        "if train_val_diff > 0.1:  \n",
        "    print(f\"Potential Overfitting Detected! Train-Validation Accuracy Difference: {train_val_diff:.4f}\")\n",
        "else:\n",
        "    print(\"No significant overfitting detected.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
